\begin{abstract}
%	Omnidirectional cameras record the 360 degree videos of real world so that they can be later experienced in AR/VR headset. As the existing OD camera systems are built from off the shelf camera devices and uses the conventional stitching algorithms we see a potential research opportunity to close gaps between hardware and software. Many existing systems like Google Jump, Facebook Surround capture and compute on enormous amount of data consuming several hundreds of watts of power to stitch images in realtime 30fps. The main challenge in OD panorama generation is to understand the data flow across the system and to make decisions on data abstractions needed at different subcomponents to reduce the total system power.
Generating real world content for VR is challenging in terms of capturing and processing at high resolution and frame-rates in real-time. The content needs to be captured for 360 and stereo view so that the experience is truly immersive. But capturing such content needs multiple cameras and involves huge amount of computation. The existing solutions only capture at the user and offloading the compute to the server. But offloading large amounts of raw camera feed takes longer latencies and poses difficulties for real-time applications. On the other-hand moving the traditional stitching algorithms to battery constrained device needs at-least three orders of reduction in power. we approach the problem by building a hardware prototype and characterize the end-to-end system bottlenecks like energy and latency. We later propose techniques for optimization.\newline


 We found that capturing is bottlenecked by data-rates across interfaces, where as compute is bottlenecked by both data rates and computations. The existing systems lacks hardware, software and algorithm co-design aspects leading to excessive data transfers across the interfaces and expensive computations within the individual sub-systems.The co-design aspects are especially pronounced at Image Signal Processor(ISP) stage. ISP is intermediate stage between the capture and compute, providing interesting data abstractions like motion vectors, bayer statistics to optimize the compute and sensor data traffic respectively. \newline
 
 We emphasize the principles of reuse to optimize the sensor and computation stage power. Reusing the previously captured data can reduce the sensor ADC power, and reusing the previous results can save significant amount of re-computations.
 
 
 
 
 
 
 %Forthe overall system, in a way to reuse the data from one stage to another so that it can reduce the computations in the we categorize the latency and energy of individual subsystems like Camera, Image Signal Processor, Processor, Memory, and Interfaces.we approach the problem by building a prototype and characterize the end-to-end system bottlenecks like energy and latency on the hardware.\newline


 

%The most computationally intensive task was finding dense optical flow, which consuming 70\% of the cpu-runtime.  We study the end-to-end data flow and recommend techniques to optimize the data flow and computations.

%
	%Omni-directional cameras capture 360 view so that they can be viewed in VR headsets. 
 	
\end{abstract}
