\begin{abstract}

Generating real-world content for VR is challenging in terms of capturing and processing at high resolution and high frame-rates. The content needs to be captured for a truly immersive experience where the user can look around in 360-degree view and perceive the depth of the scene. The existing solutions only capture and offloading the compute load to the server. But offloading large amounts of raw camera feeds takes longer latencies and poses difficulties for real-time applications.  By capturing and computing on the edge, we can closely integrate the systems and optimize for low latency, but moving the traditional stitching algorithms to battery constrained device needs at-least three orders of reduction in power. We believe that close integration of capture and compute stages will lead to synergies in terms of reduced capture, interface, and compute power. 

We approach the problem by building a hardware prototype and characterize the end-to-end system bottlenecks like power and performance. The prototype has 6 IMX274 cameras and uses Nvidia Jetson TX2 development board for capture and computation. We found that capturing is bottlenecked by sensor power and data-rates across interfaces, whereas compute by the total number of computations per frame. Our characterization shows that redundant capture and redundant computations lead to high power, huge memory footprint, and high latency. The existing systems lack hardware-software co-design aspects leading to excessive data transfers across the interfaces and expensive computations within the individual subsystems. We finally propose mechanisms to optimize the system for low power and low latency. We emphasize the importance of co-design of different subsystems to reduce and reuse the data. For example, reusing the motion vectors of the ISP stage reduces the memory footprint of the stereo correspondence stage. Our estimates show that pipelining, parallelizing on custom FPGA can achieve low latency for real time stitching.
 	
\end{abstract}
