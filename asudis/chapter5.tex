\chapter{Proposed mechanisms for energy and performance savings}

	Technique $\Rightarrow$ Benefits  \newline

1) Hardware software co-design: Re-using motion vectors generated by ISP stage to reduce the re-reference of previous image frames to calculate motion vectors for optical flow generation.\newline
(How to check if we need to compute everything with intensive vision algorithms or just use the precious results, what granularity to compute. Eg. Pyramids updating. Reactive to reconfiguration and powering on and off.)\newline
	a) Reduction of DRAM capacity requirement\newline
	b) Reduction of DRAM bandwidth requirement\newline
	c) Reducing end-to-end latency in generating dense optical flow \newline
The calculation of optical flow is the major bottleneck in terms of both memory usage and computation. The inputs for the optical flow include the current and previous camera frames of two adjacent cameras, their alpha channel and the previous optical flow. The need for temporal frames is to detect motion that is used of temporal flow regularization. Since the main memory is limited, the previous frames are usually stored in disk which increase the flow computation latency, and increase the DRAM capacity requirement and the bandwidth requirement. Instead of saving the temporal frames  for motion estimation, we can leverage the motion estimation hardware IP's that use optimal DRAM and completely remove the need to go through the disk. We model this by precomputing the motion vector and feeding with the image data. From our emulation, we found reusing the motion estimation reduces the disk utilization by \_\_ \%, the DRAM energy by \_\_ \%, improves the end-to-end latency by \_\_ \%. 
\newline
2) Data driven execution: Use of motion vectors and previous optical flow to update the pyramids to make use intrinsic properties of foreground, background and motion in the scene.
	a) Reduces the number of computations 
		i) For building pyramids.
		ii) For calculating SAD(Sum of Absolute differences) during pyramid block matching.
	b) Reduces off-chip accesses
	c) Reduce end-to-end latency
\newline

3) Hardware Accelerator:
Streaming Architecture: Using raster buffers across the entire optical flow pipeline.(Number of rows is constrained by maximum motion in the scene)
a) Helpful for scalability to higher resolution
b) Reduces size of local SRAM and off-chip memory access.\newline

	\begin{tabular}{c|c|c|c}
	Power Rail & Diff. Current(mA) & Voltage(mV) & Energy ( mJ/frame) \\
	ISP+CODEC & 102.7 & 19152 & 65.6 \\
	CPU & 16.4 & 19144 & 10.5 \\
	DDR & 260.4 & 4792 & 41.6 \\
	Camera & 375.4 & 3336 & 41.7 \\
	CPU & [] & [] & [] \\
	Accelerator[Zynq] & [] & [] & [] \\
\end{tabular} \newline 


Case for low power 360 capture. Real time Stitching 30fps >4k resolution with low power. 
Latency of GPU, CPU makes them unusable for vision tasks in AR, VR.
Case for algorithm software Co-Design for a line buffer based streaming architecture. \newline


1) Energy Characterization of end-to-end pipeline\newline
Camera, ISP, Computation \newline
Split of energy in computation \newline
\newline
2) Runtime Characterization \newline
a) End-to-end pipeline\newline
b) Split in computation execution\newline
\newline
3) Performing motion estimation prior to computation stage\newline
a) Savings in DRAM capacity, bandwidth(Normalized) \newline
b) Savings in DRAM Bandwidth\newline
c) Savings in overall energy\newline
d) End-to-end latency reduction\newline
\newline
4) Optimizing of computation in pyramids \newline
a) The execution time split for creation of pyramid, finding optical flow of pyramid, refining/updating the pyramids, upscaling the pyramid. \newline
98 percent is to generate optical flow(dense pixel correspondence). But only 20-30 percent actually needs to be recomputed.\newline
Main optical flow method time is 0.560256
Total time for entire optical flow is 0.584954   
 

 5) Sense the environment in gray scale and perform color mapping later? How much are you saving? 
 
 6) Egocentric motion 
	



