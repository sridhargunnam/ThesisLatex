\chapter{Introduction}

With the advent of AR/VR technologies there is increasing demand to create content specific for these technologies which can either be virtual or real. Virtual content is created using game engines whereas the real world content need to be captured from cameras and then processed to the format in which the content can be viewed in AR/VR. The main characteristics of this content is to provide immersive seamless experience where user can view in any direction as fell as if they were teleported to that location. In order to have such immersive experiences, we need to bridge the gaps in several domains including optics, graphics, audio and video, etc. But during this project we focus on capture systems for 360 degree video. \newline

What it means to have capture visually immersive real world scenes? Researchers \cite{cuervo2018creating} predict that we need very high resolution(16k) and framerates(120+) to make the experience indistinguishable visually from reality. Current 360 stereo video systems are used mainly designed for professional videography. They consist of several camera(18 in google's jump VR \cite{richardt2017video}) which are bulky, and capture lot of data which will later be offloaded and used to generate AR/VR videos thereby limiting their usability. But in order to easily capture and share such experiences we need also need to focus on usability and portability of the devices to make 360 video mainstream in AR/VR.

We increase the usability and portability of the 360 devices if we can capture and stitch the panorama on the same devices. Most of the software use traditional algorithms fit for offloading based approaches and doesn't consider power budget for implementing 360 capture using a low power portable device. 
360 video is essential for VR, but capturing and stitching them in real-time is limited by battery life. Even if battery technologies improve, capturing and stitching 360 video will have heating issues, thereby increasing skin temperature. In-order to tackle the challenge of capturing and stitching on same device, we study the system level bottlenecks in energy and performance by building a prototype. We characterize the system level bottlenecks in terms of energy consumption and latency. %[ [[condense this]
Our findings suggest that the main reason for the inefficiency is caused by building the system from off the shelf camera and traditional stitching algorithms. Conventional 360 degree is captured using a multi-camera rig and the expensive stitching is offloaded to powerful machines. Although some systems exist where stitching is done online, they are limited by output resolution, framerate and battery life. We show that the inefficiencies in the pipeline due to lack of hardware algorithm co-design. In this paper we study the data flow of the stitching pipeline by building a protype using 6 camera system. We analyse the energy and performance bottlenecks in the pipeline and analytically evaluate the mechanisms like using motion vectors to reduce temporal data access and computation, use raster buffers instead of full image to optimize on memory, and chip area. 
 
 Although commercial 360 degree solutions exist, they are mostly used for capture and stitching is offloaded to powerful machines. This limits the usability of 360 in VR and also portability and for heating. Our goal is therefore to build a 360 camera system that optimizes the entire pipeline both in hardware and software. We characterize the traditional pipeline and propose
%------------------------------

%Characterizing monoscopic, stereoscopic, bottlenecks, optimization

The document is organized as follows, in chapter two we discuss about the background and related work, in chapter three describe about the general stitching pipeline for VR panorama generation, and the prototype system design. In chapter four we present the evaluation results of the prototype system design. We then discuss proposed optimizations in chapter five and our discussions and conclusions in the  in chapter six.

The contributions of our work are as follows:\newline
1) Build end-to-end system for capturing 360 degree video.\newline
2) Characterize Individual Stage energy and latency and highlight the bottlenecks in the system.\newline
3) Propose architectures to optimize end-to-end data flow and data abstractions needed at sub-system level. i.e optimizing the spatio-temporal redundancies in the data and computation.\newline

%From the experiences of having see monoscopic 360 videos on samsung gear VR, I felt that complete potential of VR devices is not being utilized. Although the current “Head Mounted Devices” devices are capable of showing 360 videos with depth information, we don't have lot of content that is stereo 360. In order to make this happen we need simpler and smaller devices which should be portable for consumer use.

%This week, I started looking into literature on 360 degree systems. I found 3 works particularly interesting. In the first work “6-DOF VR Videos with a Single 360-Camera”, they use “structure from Motion” and video stabilization techniques to generate stereo 360 videos with 6DoF  from a monoscopic 360 capture device.  Other two works are from google and facebook respectively. Google's work uses 16 cameras and facebook is trying to use just 4 cameras. Both of them provide different set of challenges. I am currently reading these two works and trying to figure out the direction for my thesis work. 


%The image capture pipeline is very simple and is not much scope for novelty as the single image frame stitching pipeline in hardware is straightforward. We need to think of smart ways to scale it for video stitching in 360, especially for making the videos spatio-temporal consistency between frames when the object lying on the boundary are moving across the fisheye images. 

