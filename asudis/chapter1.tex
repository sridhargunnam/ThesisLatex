\chapter{Introduction}

From the experiences of having see monoscopic 360 videos on samsung gear VR, I felt that complete potential of VR devices is not being utilized. Although the current “Head Mounted Devices” devices are capable of showing 360 videos with depth information, we don’t have lot of content that is stereo 360. In order to make this happen we need simpler and smaller devices which should be portable for consumer use.

Current stereo 360 video systems are used mainly designed for professional videography. They consist of several camera(18 in google’s jump VR) with bulky camera rig to support them. They record lot of data which will be offloaded and used to generate stereoscopic 360 videos. 

This week, I started looking into literature on 360 degree systems. I found 3 works particularly interesting. In the first work “6-DOF VR Videos with a Single 360-Camera”, they use “structure from Motion” and video stabilization techniques to generate stereo 360 videos with 6DoF  from a monoscopic 360 capture device.  Other two works are from google and facebook respectively. Google’s work uses 16 cameras and facebook is trying to use just 4 cameras. Both of them provide different set of challenges. I am currently reading these two works and trying to figure out the direction for my thesis work. 
------------------------------------------------------------
The stitching pipeline optimization for quality have been published since last two decades. But  it is mostly on software and they are very general algorithms and doesn’t consider power budget for implementing 360 capture using a mobile device. The image capture pipeline is very simple and is not much scope for novelty as the single image frame stitching pipeline in hardware is straightforward. We need to think of smart ways to scale it for video stitching in 360, especially for making the videos spatio-temporal consistency between frames when the object lying on the boundary are moving across the fisheye images. 
------------------------------
Usecases \newline
See the commit change 2
Overview of paper \newline

use cases - mobile 360 capture, AR, VR, MR, Autonomous Driving. \newline

Usecase1: 360 stereo capture for experiencing in VR headsets. 
[xx1] Sports live stream (only few powerful ones are sufficient)
For general purpose capture and streaming, we need portable and long capture time capable. Existing VR cameras are available in small and large form factors but they have limited live streaming capabilities and don't have good battery life(60 minutes max). 
% This is mostly because they use off the shelf camera devices, and ISP's and general purpose computing like CPU, GPU in the processing pipeline. We believe that optimizing the algorithms and systems by taking the inherent qualities of video compression can remove the data access and computation redundancies. 

Usecase2: In Mixed reality headsets(military training, gaming, etc) we capture and overlay virtual objects and display. So latency critical stitching. 
%  (check examples 2 :https://en.wikipedia.org/wiki/Mixed_reality), 

Usecase3: Hybrid, 360 monoscopic and stereoscopic capture and display.

Status quo:
Existing 360 camera solutions. \newline
%\cite{Milbeaut-ISP}



Characterizing monoscopic, stereoscopic, bottlenecks, optimization


360 video is essential for VR, but capturing and stitching them in real-time is limited by battery life. Even if battery technologies improve, capturing and stitching 360 video will have heating issues, thereby increasing skin temperature. In-order to tackle the challenge of capturing and stitching on same device, we study the system level bottlenecks in energy and performance by building a prototype. Our findings suggest that the main reason for the inefficiency is caused by building the system from off the shelf camera and traditional stitching algorithms. 

Conventional 360 degree is captured using a multi-camera rig and the expensive stitching is offloaded to powerful machines. Although some systems exist where stitching is done online, they are limited by output resolution, framerate and battery life. We show that the inefficiencies in the pipeline due to lack of hardware algorithm co-design. In this paper we study the data flow of the stitching pipeline by building a protype using 6 camera system. We analyse the energy and performance bottlenecks in the pipeline and analytically evaluate the mechanisms like using motion vectors to reduce temporal data access and computation, use raster buffers instead of full image to optimize on memory, and chip area. 


Although commercial 360 degree solutions exist, they are mostly used for capture and stitching is offloaded to powerful machines. This limits the usability of 360 in VR and also portability and for heating. Our goal is therefore to build a 360 camera system that optimizes the entire pipeline both in hardware and software. We characterize the traditional pipeline and propose



