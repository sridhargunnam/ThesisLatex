\chapter{Background}
Existing systems \newline
Google Jump, Facebook Surround, Mega Stereo, Samsung Gear 360 \newline
Differentiating our work \newline
Bottlenecks in the existing systems \newline

Terminology:
Pinhole Camera
Fisheye
Spherical Projection
Equirectangular Projection
Camera Calibration Process
Intrinsics
Extrinsics
Optical Flow
Temporal Regularization
Pyramid
Video Compression: 
Stages in 3D video recording:
Capture
Data compression and transfer
Video frame Queue
Stitching precomputing
GPU vertex shader
GPU fragment shader
Processed queue 
Encoding

Image Representations:
mage representations, RGB Vs HSV Vs YCbCr(YUV)
RGB is mainly for display’s. HSV is easier as we have intensity channel is available. For vision applications intensity channel is very useful, for graphics people it’s easier as they can change the Hue - color, Saturation - colorfulness(intensity of color, shades of color), value - overall intensity  intensity of image. 

Y’CbCr is used to mainly for storage and transmission efficiency. Y’ - Luma, which is intensity of gamma corrected RGB image. 
( We use gamma correction, which is non linear encoding of luminance/brightness to adjust for the way humans perceive light and color, i.e we perceive the variations in darkness more than the variations in brightness. And also we perceive light more than color. )

The more perceivable Y’ is used with high precision and Cb Cr can be of low precision. 
-----------------------------------------
Compression Related Work: 
Conduit encoding:
https://avp.github.io/conduit/
In this  project, they are using region based compression and other smart techniques. Although these works are for VR streaming, they share common techniques like reducing bandwidth by managing the image representation and decreasing latency by close integration of stages in VR streaming.
Pyramid encoding: https://code.facebook.com/posts/1126354007399553/next-generation-video-encoding-techniques-for-360-video-and-vr/
I think facebook's pyramid encoding seems a good idea for streaming, at that scale of facebook's needs. But I have some criticism for  cube projection, as they don't use region based compression. The pyramid encoding is a good technique though, to reduce image file size


MOnoscopic Stages:
The stages may include Intensity compensation, fisheye unwarping, alignment, blending. The main goal of this work is to reduce bandwidth requirement and remove redundant computation during different stages. 

Data Flow \newline
Block Diagram
- With different stages.
- With Data Inputs and Data Outputs of each stage. (Zoom in Diagram)



Motion detection 



Data and power numbers for Google Jump VR:
Number of cameras: 17
Data Generated per each frame by 17 cameras(in bayer): 816 MB

Data bandwidth requirement in Gb/s : 47.8 Gb/s
(@30fps, compressed data(1:8 compression ratio))

Power:
Camera Capture and basic processing at camera:  50 W
DRAM power(approx.) : 61 W 
(30 W corresponds to store data generated by 30 frames of all 17 cameras)
On chip LVDS power(approx): 19 W
(For transferring one frame of all 17 cameras)
Compute Power: ( very high, done offline using multiple GPU’s)

Facebook also has 360 stereo solutions by name “facebook surround”. One of them with 24 cameras and other with 6 cameras. I may need to find data for them as well. 

The power numbers says lot of scope for optimization using the techniques we are going to propose. My work for next one week will be to understand the math for generating the two views for the stereo. 
