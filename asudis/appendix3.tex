\chapter{Systems Architecture Survey}
Heteregeneous Systems
Memory Access scheduling has been very critical since the single core processor. [5] Rixner et. al. introduced the scheduling of DRAM operations out of order, to optimize memory system performance. They reordered the memory references to exploit row buffer locality. But the major drawback of this kind of memory schedulers was they optimized for throughput rather than for the overall performance of different application threads in the system. This became more evident with the chip multiprocessors(CMP). Mutlu and Moscibroda proposed a Stall Time Fair Memory(STFM) Access Scheduling[4] to overcome these issues. STFM provides quality of service(QoS) to all the threads by measuring the slowdown of threads. The results show that slowdown related to thread level interference is significantly reduced and also, it improved average system throughput.\newline
In the last few years, hardware accelerators have dominated the SoC from mobile to High-performance computing. The modern mobile SoC’s[7, 8]  are heterogeneous architectures that integrate CPU with hardware accelerators like  GPU, image processor, audio processor. On the other hand, High-performance Computing clusters are now being accessible through cloud services like amazon web services[9] and others.  Heterogeneous System Architecture(HSA) [11] is being developed by multiple companies to integrate CPU and GPU unified virtual memory.  As a step further Unified memory is currently introduced in Nvidia CUDA 6. All of this motivates to further study on application interference, meeting QoS targets for close integration of CPUs and GPUs.\newline
The heterogeneous systems are currently being optimized to have same off-chip memory and even share ldast level Cache to reduce area, remove memory copy overheads and for tighter integration between CPU and Hardware Accelerators(HWA).  The sharing of resources lead to interference in memory scheduling if not prioritized efficiently. Since the heterogeneous systems are required to provide support to multiple application, QoS of each application becomes very critical. Several works have proposed in this area[1,2,3,12]. Staged memory scheduling [1] proposes a new approach to improve system performance and fairness, especially in integrated CPU-GPU systems. Although it provides fairness, it doesn’t take care of strict QoS requirements by applications. Jeong et al.[12] prioritize applications when the HWA’s are unable to meet the application deadlines. This mechanism doesn’t help resolve the QoR issues completely as the applications don't meet the deadline when they are prioritized very late.  Dash, Squash [2,3] propose ideas to overcome this. Their first idea, Distributed Priority, prioritize the HWA when they are distant from the deadline, thereby removing the risk of missing the deadline.  Their second idea is application aware scheduling for CPU’s.  This is based on the observation that memory intensive CPU applications are not affected when HWA’s are given priority over them. The third idea is to prioritize HWA’s with a short deadline with the highest priority when they are close to the deadline.\newline
The DASH and SQUASH schemes mainly focus on applications where there are multiple kinds of HWA’s, and there is contention for memory resources. This kind of systems is common in mobile systems. But there is another breed of heterogeneous systems which is dominated by GPU’s and CPU’s, mainly used in high-performance computing(HPC). The ideas given in Dash and SQUASH cannot be directly applied to HPC systems. Through this project, I plan to study the heterogeneous systems dominated by GPU’s. The characterization studies by [13, 14] make observations related to CPU-GPU systems.  [14] evaluates the performance seen with coarse grain and fine grain synchronization. They test two types of memory configurations, i.e separate L3 cache vs shared L3 cache. [13] concludes that shared LLC will provide better caching for heterogeneous processors compared to discrete GPU systems. So my project will focus on evaluating the performance bottlenecks and the meeting the QoS requirements for CPU-GPU based heterogeneous systems where Last Level cache is shared.\newline

References:


Experience With FPGA Vs NVIDIA Jetson: \newline
Last week’s target is to complete the Alexnet CNN layer C1 to C5 operations using DRAM based FPGA implementation. Since the run time of the tools is in the order of hours and even days(for place and route step), I planned my work as follows. I have divided my work into two parts. First one is to take simple accelerator design(AXI, AXI-lite based simple accelerator + Microblaze ) and perform  IP integration, generating the drivers files. Xilinx SDK will then be used to write c program using driver API’s to transfer data between microBlaze and AXI-based memory-mapped I/O interface of the simple accelerator.  The second task is to take the actual design C1-C5 layers of Alexnet through same steps. The first task is successful. I used Xilinx SDK tools to write simple c-codes to transfer data between microblaze and accelerator. I used axi-lite interface of the accelerator to send and receive the data using microblaze. 

The second task is to take the actual design through the same design flow and use drivers API’s to do data transfer. Before doing that I needed to rewrite the c++ code to make sure that all the data required by CNN accelerator can be received by single AXI interface(burst mode). I changed it so that accelerator will get the start pointer of array, and control signals(start, stop)  through axi-lite interface and then the accelerator will make memory requests using AXI burst mode at high data rates. I did these changes and I am going through place and route step currently. 

The tool run-time has been a big problem currently. It takes forever to complete synthesis/implementation steps. The baseline design( Microblaze + simple addition accelerator) takes 1 hour time. But the runs with actual Alexnet accelerator + microblaze doesn’t complete even after day. The runs for just C1 layers hasn’t run to complete even after one full day. I had to plan well, so that I’m not idle when the runs are happening. For example, I would work on the design interfaces, integration of IP’s, validation, and development of application in SDK, when the runs are happening.

Although there are runtime issues, others things are either figured out or in the process of implementation.  I got to know about the entire design flow and integrating accelerators, interfacing different IP modules using AXI, and hardware/software development for embedded systems. This should make my work easier for future projects that involve fpga development. 




