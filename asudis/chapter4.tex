\chapter{Characterization}

The goal of the work is to characterize the energy and latency of end-to-end Omni-directional(OD) Camera  systems both in the hardware and software pipeline and propose optimizations. As the existing OD camera systems are built from off the shelf camera devices and uses the conventional stitching algorithms, they capture, transfer redundant data and perform redundant computations. The redundancy can be attributed to the sptio-temporal correlation between the frames. The main challenge in OD panorama generation is to understand the data flow across the system and to make decisions on data abstractions needed at different subcomponents to reduce the total system power.

We characterize both monoscopic and ODS camera systems. The difference between them is the number of novel views needed is significantly higher for ODS compared to monoscopic. Both the systems at core use optical flow based stitching, so we will characterize generalized flow based stitching system and notify any important differences between monoscopic and ODS when necessary.\newline


\section{Measurement Methodology} % Energy and Latency Measurement 
Jetson has power monitor IC and ways to moniter CPU, GPU, memory frequencies. We measure the absolute energy of the system and the difference between the idle and active state for individual stages of the pipeline.
Measurement by experiments:
Jetson power monitor for camera and ISP, and Optical flow power from Zynq\newline
Analytical and Simulation based:
Micron System Power Calculator for LPDDR2. \newline
Application run time measurement.




\section{Energy Characterization}
\subsubsection{Individual stage energy}
	\begin{tabular}{c|c|c|c}
	Power Rail & Diff. Current(mA) & Voltage(mV) & Energy ( mJ/frame) \\
	Sony IMX-274 (Camera) & 375.4 & 3336 & 41.7 \\
	ISP+CODEC (TX2) & 102.7 & 19152 & 65.6 \\
	ARM-A57(Camera Cntrl.) & 16.4 & 19144 & 10.5 \\
	DRAM (8 GB LPDDR4)  & 260.4 & 4792 & 41.6 \\
	Quad ARM-A57(Stitching) & [] & [] & [] \\
	\end{tabular} \newline \newline
	The camera system is configured to 1920x1080 resolution at 30 fps for the below measurements. [make absolute energies instead of diff. i.e active -idle]\newline
	
	As seen in the above table, the processing  the optical flow implementation in software is expensive both in terms of energy and latency. Therefore, we approximate the energy and latency for FPGA based accelerator based on Xilinx's implementation of optical flow on Zynq board, discussed in chapter5.\newline

	
\section{Latency Characterization}

 Performance \newline
	a) Individual Stage Latency Characterization
	
	% Breakdown of sharpening \newline

\subsection{Individual Stage latency}
For camera system and ISP stages the latency is taken from NVIDIA camera API documentation. We measure the latency of computation in terms of CPU runtime. 

\subsection{Optical flow runtime breakdown}
Time for each Pyramid search(98\%) as shown in fig 3.1.
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=1\textwidth]{/media/gunman/Data/thesis/ThesisLatex/data/images/pyramid_runtime.png}
		\caption{X-axis shows the pyramid level and Y-axis the runtime tile search and propagate.}
		\label{fig:ex_4_9}
	\end{center}
	\vspace{-0.3in}
\end{figure} 

\section{Sensor Response Time}
Making case for hardware controller to perform camera configuration. 
Camera startup time needs to be less. 
	Initial frames are low contrast but improves over time. 
	% May be regenerate these figures
	%/media/gunman/Data/Programming/python_coding
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=1\textwidth]{/media/gunman/Data/thesis/ThesisLatex/data/images/sensor0_low_contrast.jpg}
			\caption{X-axis shows the pyramid level and Y-axis the runtime tile search and propagate.}
			\label{fig:ex_4_9}
		\end{center}
		\vspace{-0.3in}
	\end{figure}[h]

	\begin{figure*}
	\begin{center}
		\includegraphics[width=1\textwidth]{/media/gunman/Data/thesis/ThesisLatex/data/images/Before_Histogram_equilization.jpg}
		\caption{X-axis shows the pyramid level and Y-axis the runtime tile search and propagate.}
		\label{fig:ex_4_9}
	\end{center}
	\vspace{-0.3in}
\end{figure*} 

	\begin{figure*}
	\begin{center}
		\includegraphics[width=1\textwidth]{/media/gunman/Data/thesis/ThesisLatex/data/images/Normal_Histogram_Distribution.jpeg}
		\caption{Xaxis shows the pyramid level and Y-axis the runtime tile search and propagate.}
		\label{fig:ex_4_9}
	\end{center}
	\vspace{-0.3in}
\end{figure*} 


\section{Design Scalability}	

\subsubsection{Runtime scalability with the resolution}
Outputs:
4k, 6k, 8k, 12k
Discuss scalability of resolution for different stages. \newline
Especially the scalability of cache, DRAM, CPU power. 
\begin{figure*}
	\begin{center}
		\includegraphics[width=1\textwidth]{/media/gunman/Data/thesis/ThesisLatex/data/images/ExecutionTimeComputeStages.png}
		\caption{CPU execution time of different compute stages. X axis has different sub-stages in optical flow and Y axis correspond to energy per frame.}
		\label{fig:ex_4_9}
	\end{center}
	\vspace{-0.3in}
\end{figure*} 

What is the energy per each output pixel	
What is energy per pixel when generating only one ODS view, and how does it compare when generating two views! If it is double then we have a problem to solve	
[Check why sharpening is so costly!]

\subsubsection{Resource scalability with the resolution}
We measure the DRAM capacity required and bandwidth needs as we increase the resolution as a parameter for resource scalability. Higher capacity indicates the need for better encoding schemes and high bandwidth can indicate the temporal redundancy in the data, thereby increasing the bandwidth. For 3k, 4k, 6k and 8k output resolution.

Although we built a system where all the cameras are capturing at same resolution and framerate at a given time, we expect the future cameras make these decisions dynamically to save power. Therefore, we  measure the efficiency of capture and ISP processing at different modes of operation and measure the efficiency of capture and processing in power consumed per pixel at different modes.

DRAM memory size\newline
\begin{tabular}{c|c|c|c|c}
	Stage & 3k & 4k & 6k & 8k \\
	camera Input & - & - & - & - \\
	ISP & - & - & - & - \\
	Motion Estimation & - & - & - & - \\
	fish2EqRect Projection & - & - & - & - \\
	Optical Flow & - & - & - & - \\
	Sharpen & - & - & - & - \\
\end{tabular} 

\vspace{10mm}
\begin{tabular}{c|c|c|c|c}
	o/p Resolution & 3k & 4k & 6k & 8k \\
	Avg. DRAM BW & - & - & - & - \\
\end{tabular} 
\vspace{30mm}

% Figure 
\begin{figure*}
	\begin{center}
		\includegraphics[width=1\textwidth]{/media/gunman/Data/thesis/ThesisLatex/data/images/Power_Efficiency_of_Camera_ISP_Stages_in_different_configurations.png}
		\caption{Power Efficiency of Camera ISP Stages in different configurations}
		\label{fig:ex_4_9}
	\end{center}
	\vspace{-0.3in}
\end{figure*} 

Static and Dynamic Power. Resourse Utilization. 
Clock gating:
We were able to do clock gating of pixel clock. It works fine. The next step is to select between multiple clocks and see how it affect the output image.

Latency:
By varying the latency of reconfiguration, the image is not stable. It shows horizontal and vertical bars of image. This has to be fixed in this week. 


\section{Evaluating data-flow redundancies}
Evaluating redundant computations in optical flow. 
Accuracy Vs Energy\&Perf tradeoff. 
Next we evaluate the optical flow for a scene where foreground has movement and background is static. The temporal flow difference is found out to see the variation of flow. As expected we see the flow change only for the regions where the objects are moving. This observation suggests that accuracy of optical flow can be trade-off with computation time to save energy and latency. It also shows that the accuracy drop is less than \_ percent which can be acceptable


\section{Misc}

	
Increased frame-rate\newline
What is the percentage of new data
ffmpeg I-frame size to the P-frame ratio.
\begin{figure*}
	\begin{center}
		\includegraphics[width=1\textwidth]{/media/gunman/Data/thesis/ThesisLatex/data/images/FramesizevsFramenumber.png}
		\caption{Framesize of I and P frames}
		\label{fig:ex_4_9}
	\end{center}
	\vspace{-0.3in}
\end{figure*} 
%ffprobe lab.mp4 -show_frames
we usually have I,P, and B frames in a compressed video, but in case of realtime compression we will have only I and P frames and disable B frames as it adds latency to the pipeline. Typically I-frame to P-frame is about ~4 times and one I-frame occur for ~30 P-frames. This implies we save a lot on interface power if we can push the computation to near sensor.  
[Numbers - savings from IO datarate reduction]

3) Survey of Camera and ISP stage energy breakdown
Camera Sensor and ISP power directly taken from literature.
Computation Power split into sub stages. \newline
For power characterization of camera sensor and ISP, we run the camera in different resolutions and framerates and see how the various sub-component power changes. The components include Camera Sensor, I/O, ISP, CODEC, DDR, and CPU. The ISP, and CODEC power are combined as they belong to same SOC voltage rail.

The most used configuration for our project when all the six cameras are capturing 1920x1080 @ 30 fps. At this configuration below is the split of different component power. \newline


	e) Quality Tradeoff's with input resolution\newline
Sharpening
Reduction in fidelity of unwarped image, as interpolation is not being done.
Reducing number of pyramid's
f) High motion Vs low motion differences. Size of motion vector to that of size of full frame. 
g) File IO power
h) Breakdown in terms of type of Memory	used
i) Breakdown in terns of type of Computation
j) Breakdown in terms of IO bandwidth bottlenecks