\chapter{Proposed Optimizations}
\label{chap:PropOpt}
Based on the characterization of end-to-end system pipeline we can group the proposed optimizations into two categories, i.e, to optimize for low power and another for low latency. For low power we talk about techniques to reduce sensor power and then about spatio-temporal data reuse during computation. For reducing the compute latency we talk about use of pipelining and parallelism and the expected benefits. 

\subsection{Techniques for Low Power}

\subsubsection{Reducing the ADC Power}
In a multi-camera system, the combined power of sensors becomes a critical bottleneck. Most typical image sensors are not intelligent to compute over the only the changing pixels. The temporal frames have high amount of redundancy, when there is less motion of camera and/or the objects in the scene. Therefore it is possible to have different modes of camera operation through which we can save power. The different modes could be reducing the resolution and frame-rates, which are explored in previous works \cite{hu2018characterizing}. In addition to such optimizations, we propose the adaptability of camera's ADC circuitry to save power even further. 

A camera capturing 4k, 30fps typically consumes ~500mW of power, and the ADC contributes to ~60\% of total sensor power. We observed the dynamic range is smaller for local regions which in frame, i.e, the pixels will have same most significant bits (MSB) and only vary for the least significant bits (LSB). This information can be used to reduce the number of ADC conversion cycles in a SAR like ADC's. The conversions can only start for the last few bits thereby reducing ADC conversion cycles by more than 50\%. It should also be noted that the voltage swings for resolving MSB bits is higher than the LSB bits. Therefore there is extra savings in terms of reducing the voltage swing. But when the predicted starting point of ADC conversion goes wrong, the conversion should start for MSB bit. 

\subsubsection{Reducing Camera Serial Interface Power}
The sensor interface power consumes about 17\% of total system power during capture. Currently the interfaces stream the raw camera data to ISP. But this power can also be reduced by using light weight compression techniques and encoding using Huffman encoding. But in order to be able to use Huffman encoding, we need the entropy of the data. As the recent ISPs have a Bayer statistics accelerator embedded inside them, we can easily generate the approximate entropy of frame for various regions. Such co-design techniques should drastically reduce the interface power and even enhance the effective number of pixels transmitted for a given channel bandwidth. 

\subsubsection{Improving the data abstractions across sub-systems}
The existing system pipeline consist of multiple subsystems that are not optimized for data sharing to optimize the system globally. For example, the motion vectors are computed during ISP stage, but as there results are abstracted away from  other blocks like compute stage, it increase the overall memory footprint and extra computation cycle in compute stage. Similarly, the image statistics generated by ISP stage can be used for optimizing the sensor and sensor interface power. Sensor power can be optimized by providing the region of pixel values for the ADC, and entropy information of frame statistics at ISP can be used to encode the interface power without extra cost for generating the entropy information at the sensor end. 

\subsection{Techniques for Low Latency}

The camera and ISP stages are optimized in hardware generating realtime inputs needed for ODS stitching. So the main contributor for latency is the computation of dense stereo correspondence and the view synthesis stages. We estimate the latency reduction of applying the techniques like pipelining and parallelization in the following sub-sections. \vspace{-0.3cm}
\subsubsection{Pipelined and Parallelized Execution}
The main source for high latency is the sequential execution of different stages and sub-stages of the stitching pipeline. The stitching pipeline consist of stages like projection, optical flow and view synthesis which are sequentially executed. Pipelining the stages by would bring the latency to max of the latencies of the individual stages, which is ~16 sec of the optical flow. The optical flow again consist of computing results for multiple pyramids, which maximum latency of higher level of pyramid comes around 0.1 sec. By pipelining we can bring down the latency from 16 seconds to 0.1 second. We estimate these numbers from \cite{denseOFlowXilinx} implementation of dense stereo optical flow reference design. In the next section we discuss how to go from 0.1 sec to real time in the range of 30 fps. 

All the tasks in existing software pipeline can be accelerated using multiple cores. The dense optical flow for 4k resolution on FPGA takes only 17 milliseconds compared to 16 seconds by CPU implementation. We use \cite{4k60fpsZynqFPGA} to approximate the compute power for ODS generation at 6k resolution and 30 fps, which is 5.4 Watt. An ASIC implementation of the same design should further bring down the power to the order of 1 Watt. Therefore, the total system power including capture will be in the range of 3-5 Watts for 6k output ODS resolution.

